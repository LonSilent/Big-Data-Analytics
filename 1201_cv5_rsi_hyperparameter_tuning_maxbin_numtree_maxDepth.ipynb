{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bb2efe05b86a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[0mRSI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mRSI_H\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-bb2efe05b86a>\u001b[0m in \u001b[0;36mRSI\u001b[1;34m(data, day)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mbolRise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m#print(bolRise)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mmeanRise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbolRise\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmeanRise\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1971\u001b[0m         \u001b[1;31m# shortcut if we are an actual column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1972\u001b[1;33m         \u001b[0mis_mi_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1973\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_mi_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import time\n",
    "data = []\n",
    "datapath = '/home/bigdatas16/1201.csv'\n",
    "data = pd.read_csv(datapath)\n",
    " \n",
    "# Close Rise Ratio 漲幅比\n",
    "def RR(data):\n",
    "    dataList = range(data['Date'].size)\n",
    "    tmpList = []\n",
    "    #tmpList.append(0)\n",
    "\n",
    "    for item in dataList:\n",
    "        # 防止 第一筆data沒有更舊的\n",
    "        if item - 1 >=0:\n",
    "            # (今日收盤價 - 昨日收盤價)/昨日收盤價\n",
    "            tmp = (data['Close'][item]-data['Close'][item-1])/data['Close'][item-1]*100\n",
    "            tmpList.append(tmp)\n",
    "        elif item - 1 < 0:\n",
    "            tmp = 0\n",
    "            tmpList.append(tmp)\n",
    "        \n",
    "    # 前day 沒data會出現NA\n",
    "    tmpSeries = pd.Series(tmpList)\n",
    "    # create  RR 欄位\n",
    "    data['RR']=tmpSeries\n",
    "RR(data)\n",
    "# 相對強弱指標(RSI) 建議6\n",
    "def RSI(data,day):\n",
    "    dataList = range(data['Date'].size)\n",
    "    tmpList = []\n",
    "    #tmpList.append(0)\n",
    "    for item in dataList:\n",
    "        # 防止前day沒有data\n",
    "        if item - day >= 0:\n",
    "            # 6日RSI=100*6日內收盤上漲總幅度平均值 / (6日內收盤上漲總幅度平均值 - 6日內收盤下跌總幅度平均值)   \n",
    "            bolRise = data['RR'][item-day+1-1:item+1-1] > 0\n",
    "            #print(bolRise)\n",
    "            meanRise = data['RR'][item-day+1-1:item+1-1][bolRise].mean()\n",
    "            \n",
    "            if meanRise > 0:\n",
    "                meanRise = meanRise\n",
    "            else:\n",
    "                meanRise = 0\n",
    "                \n",
    "            bolDesc = data['RR'][item-day+1-1:item+1-1] < 0\n",
    "            #print(bolDesc)\n",
    "            meanDesc = data['RR'][item-day+1-1:item+1-1][bolDesc].mean() \n",
    "                \n",
    "            if meanDesc < 0:\n",
    "                meanDesc = meanDesc\n",
    "            else:\n",
    "                meanDesc = 0\n",
    "                \n",
    "            #print(meanRise) \n",
    "            #print(meanDesc)\n",
    "\n",
    "            if meanRise == 0 and meanDesc == 0:\n",
    "                tmp = 0.50\n",
    "            else:    \n",
    "                tmp = 100 * ((meanRise*1.0) / (meanRise - meanDesc))\n",
    "            #print(tmp)\n",
    "            tmpList.append(tmp)\n",
    "            \n",
    "        elif item - day < 0:\n",
    "            tmp = 0\n",
    "            tmpList.append(tmp)              \n",
    "    tmpSeries = pd.Series(tmpList)\n",
    "    data['RSI'+str(day)] = tmpSeries\n",
    "\n",
    "# High Rise Ratio 漲幅比\n",
    "def RR_H(data):\n",
    "    dataList = range(data['Date'].size)\n",
    "    tmpList = []\n",
    "    #tmpList.append(0)\n",
    "    for item in dataList:\n",
    "        if item - 1 >=0:\n",
    "            tmp = (data['High'][item]-data['High'][item-1])/data['High'][item-1]*100\n",
    "            tmpList.append(tmp)\n",
    "        elif item - 1 < 0:\n",
    "            tmp = 0\n",
    "            tmpList.append(tmp)\n",
    "        \n",
    "    # 前day 沒data會出現NA\n",
    "    tmpSeries = pd.Series(tmpList)\n",
    "    # create  RR 欄位\n",
    "    data['RR_H']=tmpSeries\n",
    "RR_H(data)\n",
    "# 相對強弱指標(RSI) 建議6\n",
    "def RSI_H(data,day):\n",
    "    dataList = range(data['Date'].size)\n",
    "    tmpList = []\n",
    "    #tmpList.append(0)\n",
    "    for item in dataList:\n",
    "        if item - day >= 0:\n",
    "            bolRise = data['RR_H'][item-day+1-1:item+1-1] > 0\n",
    "            #print(bolRise)\n",
    "            meanRise = data['RR_H'][item-day+1-1:item+1-1][bolRise].mean()\n",
    "            \n",
    "            if meanRise > 0:\n",
    "                meanRise = meanRise\n",
    "            else:\n",
    "                meanRise = 0\n",
    "                \n",
    "            bolDesc = data['RR_H'][item-day+1-1:item+1-1] < 0\n",
    "            #print(bolDesc)\n",
    "            meanDesc = data['RR_H'][item-day+1-1:item+1-1][bolDesc].mean() \n",
    "                \n",
    "            if meanDesc < 0:\n",
    "                meanDesc = meanDesc\n",
    "            else:\n",
    "                meanDesc = 0\n",
    "                \n",
    "            #print(meanRise) \n",
    "            #print(meanDesc)\n",
    "\n",
    "            if meanRise == 0 and meanDesc == 0:\n",
    "                tmp = 0.50\n",
    "            else:    \n",
    "                tmp = 100 * ((meanRise*1.0) / (meanRise - meanDesc))\n",
    "            #print(tmp)\n",
    "            tmpList.append(tmp)\n",
    "            \n",
    "        elif item - day < 0:\n",
    "            tmp = 0\n",
    "            tmpList.append(tmp)              \n",
    "    tmpSeries = pd.Series(tmpList)\n",
    "    \n",
    "    # create  RSI 欄位\n",
    "    data['RSI_H'+str(day)] = tmpSeries\n",
    "    \n",
    "start = time.time()       \n",
    "for i in range(2,18):\n",
    "    RSI(data,i)\n",
    "for i in range(2,18):\n",
    "    RSI_H(data,i)\n",
    "end = time.time()\n",
    "print \"Create features rsi : Time taken = %f second\"%(end - start)\n",
    "\n",
    "#data = data.drop(data.index[4620])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UP_DOWN(data):\n",
    "    data_day_number = range(data['Date'].size)\n",
    "    tmpList = []\n",
    "    \n",
    "    for item in data_day_number:\n",
    "        spread = data['Close'][item] - data['Open'][item]\n",
    "        if spread > 0 :\n",
    "            tmp = 1\n",
    "        elif spread <= 0:\n",
    "            tmp = 0\n",
    " \n",
    "        tmpList.append(tmp)\n",
    "    \n",
    "    tmpSeries = pd.Series(tmpList)\n",
    "    data['UP_DOWN']=tmpSeries\n",
    "UP_DOWN(data)\n",
    "\n",
    "data1 = data.drop(['Date', 'Open', 'High','Low','Close','Volume_n','Volume_m','return','PE','RR','RR_H'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cols = data1.columns.tolist()\n",
    "#cols[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = data1.fillna(-1)\n",
    "cols = data1.columns.tolist()\n",
    "cols[48]\n",
    "data1 = data1[[cols[48]] + cols[0:47]]\n",
    "n = len(data['Date'])/5\n",
    "m = len(data['Date']) - n\n",
    "train_data = data1.iloc[:m,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import time \n",
    "sql_sc = SQLContext(sc)\n",
    "train_data.iloc[:(m/5),:]\n",
    "train_data.iloc[(m/5):(2*m/5),:]\n",
    "train_data.iloc[(2*m/5):(3*m/5),:]\n",
    "train_data.iloc[(3*m/5):(5*m/5),:]\n",
    "train_data.iloc[(4*m/5):m,:]\n",
    "\n",
    "cross_data_list = [i for i in range(1,6)]\n",
    "cross_data_list[0] = train_data.iloc[:(m/5),:]\n",
    "cross_data_list[1] = train_data.iloc[(m/5):(2*m/5),:]\n",
    "cross_data_list[2] = train_data.iloc[(2*m/5):(3*m/5),:]\n",
    "cross_data_list[3] = train_data.iloc[(3*m/5):(5*m/5),:]\n",
    "cross_data_list[4] = train_data.iloc[(4*m/5):m,:]\n",
    "cross_data_pandas = [i for i in range(1,6)]\n",
    "for i in range(0,5,1):\n",
    "    cross_data_pandas[i] = pd.DataFrame(cross_data_list[i])\n",
    "#cross_data_pandas[4]\n",
    "\n",
    "cross_sql = [i for i in range(1,6)]\n",
    "for i in range(0,5,1):\n",
    "    cross_sql[i] = sql_sc.createDataFrame(cross_data_pandas[i])\n",
    "\n",
    "assembler1 = VectorAssembler(inputCols=[\"RSI2\",\"RSI3\",\"RSI4\",\"RSI5\",\"RSI6\",\"RSI7\",\"RSI8\"\n",
    "                                       ,\"RSI9\",\"RSI10\",\"RSI11\",\"RSI12\",\"RSI13\",\"RSI14\",\"RSI15\"\n",
    "                                       ,\"RSI17\",\"RSI_H2\",\"RSI_H3\",\"RSI_H4\",\"RSI_H5\",\"RSI_H6\"\n",
    "                                       ,\"RSI_H7\",\"RSI_H8\",\"RSI_H9\",\"RSI_H10\",\"RSI_H11\",\"RSI_H12\"\n",
    "                                       ,\"RSI_H13\",\"RSI_H14\",\"RSI_H15\",\"RSI_H16\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparametr tuning cross validation 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import time \n",
    "sql_sc = SQLContext(sc)\n",
    "train_data.iloc[:(m/5),:]\n",
    "train_data.iloc[(m/5):(2*m/5),:]\n",
    "train_data.iloc[(2*m/5):(3*m/5),:]\n",
    "train_data.iloc[(3*m/5):(5*m/5),:]\n",
    "train_data.iloc[(4*m/5):m,:]\n",
    "\n",
    "cross_data_list = [i for i in range(1,6)]\n",
    "cross_data_list[0] = train_data.iloc[:(m/5),:]\n",
    "cross_data_list[1] = train_data.iloc[(m/5):(2*m/5),:]\n",
    "cross_data_list[2] = train_data.iloc[(2*m/5):(3*m/5),:]\n",
    "cross_data_list[3] = train_data.iloc[(3*m/5):(5*m/5),:]\n",
    "cross_data_list[4] = train_data.iloc[(4*m/5):m,:]\n",
    "cross_data_pandas = [i for i in range(1,6)]\n",
    "for i in range(0,5,1):\n",
    "    cross_data_pandas[i] = pd.DataFrame(cross_data_list[i])\n",
    "#cross_data_pandas[4]\n",
    "\n",
    "cross_sql = [i for i in range(1,6)]\n",
    "for i in range(0,5,1):\n",
    "    cross_sql[i] = sql_sc.createDataFrame(cross_data_pandas[i])\n",
    "\n",
    "\n",
    "assembler1 = VectorAssembler(inputCols=[\"RSI2\",\"RSI3\",\"RSI4\",\"RSI5\",\"RSI6\",\"RSI7\",\"RSI8\"\n",
    "                                       ,\"RSI9\",\"RSI10\",\"RSI11\",\"RSI12\",\"RSI13\",\"RSI14\",\"RSI15\"\n",
    "                                       ,\"RSI17\",\"RSI_V2\",\"RSI_V3\",\"RSI_V4\",\"RSI_V5\",\"RSI_V6\"\n",
    "                                       ,\"RSI_V7\",\"RSI_V8\",\"RSI_V9\",\"RSI_V10\",\"RSI_V11\",\"RSI_V12\"\n",
    "                                       ,\"RSI_V13\",\"RSI_V14\",\"RSI_V15\",\"RSI_V16\",\"RSI_H2\",\"RSI_H3\"\n",
    "                                       ,\"RSI_H4\",\"RSI_H5\",\"RSI_H6\",\"RSI_H7\",\"RSI_H8\",\"RSI_H9\"\n",
    "                                       ,\"RSI_H10\",\"RSI_H11\",\"RSI_H12\",\"RSI_H13\",\"RSI_H14\",\"RSI_H15\"\n",
    "                                       ,\"RSI_H16\"], outputCol=\"features\")\n",
    "for i in range(0,5,1):\n",
    "    cross_sql[i] = assembler1.transform(cross_sql[i])\n",
    "    \n",
    "frames = [i for i in range(1,6)]\n",
    "for i in range(0,5):\n",
    "    #print(i)\n",
    "    frames[i] = pd.concat([cross_data_pandas[(i+1)%5],cross_data_pandas[(i+2)%5],cross_data_pandas[(i+3)%5],cross_data_pandas[(i+4)%5]])\n",
    "    result_sql = [i for i in range(1,6)]\n",
    "\n",
    "for i in range(0,5):   \n",
    "    result_sql[i] = sql_sc.createDataFrame(frames[i])   \n",
    "    \n",
    "assembler1 = VectorAssembler(inputCols=[\"RSI2\",\"RSI3\",\"RSI4\",\"RSI5\",\"RSI6\",\"RSI7\",\"RSI8\"\n",
    "                                       ,\"RSI9\",\"RSI10\",\"RSI11\",\"RSI12\",\"RSI13\",\"RSI14\",\"RSI15\"\n",
    "                                       ,\"RSI17\",\"RSI_V2\",\"RSI_V3\",\"RSI_V4\",\"RSI_V5\",\"RSI_V6\"\n",
    "                                       ,\"RSI_V7\",\"RSI_V8\",\"RSI_V9\",\"RSI_V10\",\"RSI_V11\",\"RSI_V12\"\n",
    "                                       ,\"RSI_V13\",\"RSI_V14\",\"RSI_V15\",\"RSI_V16\",\"RSI_H2\",\"RSI_H3\"\n",
    "                                       ,\"RSI_H4\",\"RSI_H5\",\"RSI_H6\",\"RSI_H7\",\"RSI_H8\",\"RSI_H9\"\n",
    "                                       ,\"RSI_H10\",\"RSI_H11\",\"RSI_H12\",\"RSI_H13\",\"RSI_H14\",\"RSI_H15\"\n",
    "                                       ,\"RSI_H16\"],outputCol=\"features\")\n",
    "for i in range(0,5):  \n",
    "    result_sql[i] = assembler1.transform(result_sql[i])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "cv5_test_error = []    \n",
    "maxBins_numTrees_maxDepth = []    \n",
    "for l in range(15,35,5):\n",
    "    for j in range(1,8,1):\n",
    "        for k in range(1,10,1):\n",
    "            Test_Error = []\n",
    "            for i in range(0,5):\n",
    "                labelIndexer = StringIndexer(inputCol = \"UP_DOWN\", outputCol=\"indexedLabel\").fit(result_sql[i])\n",
    "                featureIndexer = VectorIndexer(inputCol = \"features\", outputCol=\"indexedFeatures\").fit(result_sql[i])\n",
    "                #rf = RandomForestClassifier(labelCol=\"labelIndexer\", featuresCol=\"features\")\n",
    "                rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\",numTrees = k,maxDepth = j,maxBins = l)\n",
    "                # Chain indexers and forest in a Pipeline\n",
    "                pipeline = Pipeline(stages=[labelIndexer,featureIndexer,rf])\n",
    "                model = pipeline.fit(result_sql[i])\n",
    "                predictions = model.transform(cross_sql[i])\n",
    "                evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\"\n",
    "                                                              , predictionCol=\"prediction\",metricName = \"precision\")\n",
    "                accuracy = evaluator.evaluate(predictions)\n",
    "                print(\"[%g,%g,%g] : Test Error(%d) = %g\" % (k,j,l,i,1.0 - accuracy))\n",
    "                Test_Error.append(1.0 - accuracy)\n",
    "            cv5_test_error.append(mean(Test_Error))\n",
    "            maxBins_numTrees_maxDepth.append([k,j,l])\n",
    "\n",
    "end = time.time()\n",
    "print \"Cross Validation 5 Folds Hyperparameter Tuning : Time taken = %f second\"%(end - start)\n",
    "        \n",
    "    \n",
    "mintest = np.min(cv5_test_error)\n",
    "c = []\n",
    "for i in range(len(cv5_test_error)):\n",
    "    c.append(np.min(cv5_test_error) == cv5_test_error[i])\n",
    "    \n",
    "for i in range(len(cv5_test_error)):    \n",
    "    if c[i] == True:\n",
    "        print(maxBins_numTrees_maxDepth[i],mintest)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = data1.iloc[:m,:]\n",
    "train_sql = sql_sc.createDataFrame(train_data)\n",
    "test_data = data1.iloc[m:,:]\n",
    "test_sql = sql_sc.createDataFrame(test_data)\n",
    "\n",
    "assembler1 = VectorAssembler(inputCols=[\"RSI2\",\"RSI3\",\"RSI4\",\"RSI5\",\"RSI6\",\"RSI7\",\"RSI8\"\n",
    "                                       ,\"RSI9\",\"RSI10\",\"RSI11\",\"RSI12\",\"RSI13\",\"RSI14\",\"RSI15\"\n",
    "                                       ,\"RSI17\",\"RSI_V2\",\"RSI_V3\",\"RSI_V4\",\"RSI_V5\",\"RSI_V6\"\n",
    "                                       ,\"RSI_V7\",\"RSI_V8\",\"RSI_V9\",\"RSI_V10\",\"RSI_V11\",\"RSI_V12\"\n",
    "                                       ,\"RSI_V13\",\"RSI_V14\",\"RSI_V15\",\"RSI_V16\",\"RSI_H2\",\"RSI_H3\"\n",
    "                                       ,\"RSI_H4\",\"RSI_H5\",\"RSI_H6\",\"RSI_H7\",\"RSI_H8\",\"RSI_H9\"\n",
    "                                       ,\"RSI_H10\",\"RSI_H11\",\"RSI_H12\",\"RSI_H13\",\"RSI_H14\",\"RSI_H15\"\n",
    "                                       ,\"RSI_H16\"],outputCol=\"features\")\n",
    "train_sql = assembler1.transform(train_sql)\n",
    "labelIndexer = StringIndexer(inputCol = \"UP_DOWN\", outputCol=\"indexedLabel\").fit(train_sql)\n",
    "featureIndexer = VectorIndexer(inputCol = \"features\", outputCol=\"indexedFeatures\").fit(train_sql)\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\",numTrees = 7,maxDepth = 7,maxBins = 30)\n",
    "pipeline = Pipeline(stages=[labelIndexer,featureIndexer,rf])\n",
    "model = pipeline.fit(train_sql)\n",
    "assembler = VectorAssembler(inputCols=[\"RSI2\",\"RSI3\",\"RSI4\",\"RSI5\",\"RSI6\",\"RSI7\",\"RSI8\"\n",
    "                                       ,\"RSI9\",\"RSI10\",\"RSI11\",\"RSI12\",\"RSI13\",\"RSI14\",\"RSI15\"\n",
    "                                       ,\"RSI17\",\"RSI_V2\",\"RSI_V3\",\"RSI_V4\",\"RSI_V5\",\"RSI_V6\"\n",
    "                                       ,\"RSI_V7\",\"RSI_V8\",\"RSI_V9\",\"RSI_V10\",\"RSI_V11\",\"RSI_V12\"\n",
    "                                       ,\"RSI_V13\",\"RSI_V14\",\"RSI_V15\",\"RSI_V16\",\"RSI_H2\",\"RSI_H3\"\n",
    "                                       ,\"RSI_H4\",\"RSI_H5\",\"RSI_H6\",\"RSI_H7\",\"RSI_H8\",\"RSI_H9\"\n",
    "                                       ,\"RSI_H10\",\"RSI_H11\",\"RSI_H12\",\"RSI_H13\",\"RSI_H14\",\"RSI_H15\"\n",
    "                                       ,\"RSI_H16\"],outputCol=\"features\")\n",
    "test_sql = assembler.transform(test_sql)\n",
    "predictions = model.transform(test_sql)\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"precision\")\n",
    "#accuracy = evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([\"RSI2\",\"RSI3\",\"RSI4\",\"RSI5\",\"RSI6\",\"RSI7\",\"RSI8\"\n",
    ",\"RSI9\",\"RSI10\",\"RSI11\",\"RSI12\",\"RSI13\",\"RSI14\",\"RSI15\"\n",
    ",\"RSI17\",\"RSI_V2\",\"RSI_V3\",\"RSI_V4\",\"RSI_V5\",\"RSI_V6\"\n",
    ",\"RSI_V7\",\"RSI_V8\",\"RSI_V9\",\"RSI_V10\",\"RSI_V11\",\"RSI_V12\"\n",
    ",\"RSI_V13\",\"RSI_V14\",\"RSI_V15\",\"RSI_V16\",\"RSI_H2\",\"RSI_H3\"\n",
    ",\"RSI_H4\",\"RSI_H5\",\"RSI_H6\",\"RSI_H7\",\"RSI_H8\",\"RSI_H9\"\n",
    ",\"RSI_H10\",\"RSI_H11\",\"RSI_H12\",\"RSI_H13\",\"RSI_H14\",\"RSI_H15\",\"RSI_H16\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.stages[2]._call_java('toDebugString'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    #tick_marks = np.arange(len(iris.target_names))\n",
    "    #plt.xticks(tick_marks, iris.target_names, rotation=45)\n",
    "    #plt.yticks(tick_marks, iris.target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_1101 = predictions.select(\"prediction\", \"indexedLabel\")\n",
    "prediction_1101_pandas = prediction_1101.toPandas()\n",
    "cm = confusion_matrix(prediction_1101_pandas['indexedLabel'], prediction_1101_pandas['prediction'])\n",
    "pd.crosstab(prediction_1101_pandas['indexedLabel'], prediction_1101_pandas['prediction'], rownames=['actual'], colnames=['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    #tick_marks = np.arange(len(iris.target_names))\n",
    "    #plt.xticks(tick_marks, iris.target_names, rotation=45)\n",
    "    #plt.yticks(tick_marks, iris.target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_p = data[['Close','Open']][(4621-n):4622].values\n",
    "data_p1 = pd.DataFrame(data_p,columns = ['Close','Open'])\n",
    "result = np.c_[prediction_1101_pandas['indexedLabel'],prediction_1101_pandas['prediction']]\n",
    "result1 = pd.DataFrame(result , columns = ['test', 'prediction'])\n",
    "result2 = pd.concat([data_p1, result1], axis=1)\n",
    "result2[\"Cl - Op\"] = result2['Close'] - result2['Open']\n",
    "result2['prediction'] = result2['prediction'].replace(0,-1)\n",
    "result2['profit'] = 1000*result2['Open'] * result2['Cl - Op'] * result2['prediction']\n",
    "a = result2['profit'].cumsum()\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(10.5, 8.5)\n",
    "a.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result2['processing fee1'] = np.where(result2['prediction'] == 1,1000*(result2['Close']*(4.425/1000.0) + result2['Open']*(1.425/1000.0))*1\n",
    "                                      ,1000*(result2['Close']*(1.425/1000.0) + result2['Open']*(4.425/1000.0)))\n",
    "result2['profit_r_fee'] = result2['profit'] -  result2['processing fee1']\n",
    "b = result2['profit_r_fee'].cumsum()\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(10.5, 8.5)\n",
    "b.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_p = data[['Close','Open']][(4621-n):4622].values\n",
    "data_p1 = pd.DataFrame(data_p,columns = ['Close','Open'])\n",
    "result = np.c_[prediction_1101_pandas['indexedLabel'],prediction_1101_pandas['prediction']]\n",
    "result1 = pd.DataFrame(result , columns = ['test', 'prediction'])\n",
    "result2 = pd.concat([data_p1, result1], axis=1)\n",
    "result2[\"Cl - Op\"] = result2['Close'] - result2['Open']\n",
    "result2['prediction'] = result2['prediction'].replace(0,-1)\n",
    "result2['profit'] = 1000*result2['Open'] * result2['Cl - Op'] * result2['prediction']\n",
    "data_PE = data[['PE']][(4621-n):4622].values\n",
    "data_PE = pd.DataFrame(data_PE,columns = ['PE'])\n",
    "result2 = pd.concat([data_PE, result2], axis=1)\n",
    "result2['profit2'] = 0.0\n",
    "result2['profit2'][0] = 0 \n",
    "for i in range(5,len(result2)-1,1):\n",
    "    if result2['prediction'][i+1] == 1 and result2['prediction'][i] == -1: #and result2['PE'][i] < result2['PE'][i-5:i-1].mean():\n",
    "        result2['profit2'][i+1] = result2['profit2'][i] + 1000 * (-1) * result2['Open'][i+1]\n",
    "    elif result2['prediction'][i+1] == 1 and result2['prediction'][i] == 1:\n",
    "        result2['profit2'][i+1] = result2['profit2'][i] \n",
    "    elif result2['prediction'][i+1] == -1 and result2['prediction'][i] == 1: #and result2['PE'][i] > result2['PE'][i-5:i-1].mean():\n",
    "        result2['profit2'][i+1] = result2['profit2'][i] + 1000 * result2['Open'][i+1]\n",
    "    elif result2['prediction'][i+1] == -1 and result2['prediction'][i] == -1:\n",
    "        result2['profit2'][i+1] = result2['profit2'][i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result2['profit2'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result2['processing_fee_2'] = 0.0\n",
    "result2['processing_fee_2'][0] = 0#(-1) * result2['Open'][0]\n",
    "\n",
    "for i in range(5,len(result2)-1,1):\n",
    "    if result2['prediction'][i+1] == 1 and result2['prediction'][i] == -1: #and result2['PE'][i] < result2['PE'][i-5:i-1].mean():\n",
    "        result2['processing_fee_2'][i+1] = result2['processing_fee_2'][i] + 1000 * (-1) * result2['Open'][i+1] - (1000 * result2['Open'][i+1])*((1.425/1000.0))\n",
    "    elif result2['prediction'][i+1] == 1 and result2['prediction'][i] == 1:\n",
    "        result2['processing_fee_2'][i+1] = result2['processing_fee_2'][i]\n",
    "    elif result2['prediction'][i+1] == -1 and result2['prediction'][i] == 1: #and result2['PE'][i] > result2['PE'][i-5:i-1].mean():\n",
    "        result2['processing_fee_2'][i+1] = result2['processing_fee_2'][i] + 1000 * result2['Open'][i+1] - (1000 * result2['Open'][i+1])*(4.425/1000.0)\n",
    "    elif result2['prediction'][i+1] == -1 and result2['prediction'][i] == -1:\n",
    "        result2['processing_fee_2'][i+1] = result2['processing_fee_2'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result2['processing_fee_2'].plot()#[1101:2301].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
